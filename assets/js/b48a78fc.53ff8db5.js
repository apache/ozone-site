"use strict";(self.webpackChunkdoc_site=self.webpackChunkdoc_site||[]).push([[67632],{62392:(e,t,o)=>{o.d(t,{R:()=>a,x:()=>r});var n=o(30758);const s={},i=n.createContext(s);function a(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(i.Provider,{value:t},e.children)}},72239:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"system-internals/features/om-bootstrapping-with-snapshots","title":"OM Bootstrapping with Snapshots","description":"Problem Statement","source":"@site/docs/07-system-internals/07-features/08-om-bootstrapping-with-snapshots.md","sourceDirName":"07-system-internals/07-features","slug":"/system-internals/features/om-bootstrapping-with-snapshots","permalink":"/docs/next/system-internals/features/om-bootstrapping-with-snapshots","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/ozone-site/tree/master/docs/07-system-internals/07-features/08-om-bootstrapping-with-snapshots.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_label":"OM Bootstrapping with Snapshots"},"sidebar":"defaultSidebar","previous":{"title":"Improve Ozone Snapshot Scale with Snapshot Defragmentation","permalink":"/docs/next/system-internals/features/improve-ozone-snapshot-scale-with-snapshot-defragmentation"},"next":{"title":"Developer Guide","permalink":"/docs/next/developer-guide/"}}');var s=o(86070),i=o(62392);const a={sidebar_label:"OM Bootstrapping with Snapshots"},r="OM Bootstrapping with Snapshots",l={},h=[{value:"Problem Statement",id:"problem-statement",level:2},{value:"Background on Snapshots",id:"background-on-snapshots",level:2},{value:"Snapshot Operations",id:"snapshot-operations",level:3},{value:"Current Bootstrap Model",id:"current-bootstrap-model",level:3},{value:"Workflow",id:"workflow",level:4},{value:"Issues with the Current Model",id:"issues-with-the-current-model",level:4},{value:"Proposed Fixes",id:"proposed-fixes",level:2},{value:"Locking the Snapshot Cache",id:"locking-the-snapshot-cache",level:3},{value:"Approach 1 (Batching files over multiple tarballs)",id:"approach-1-batching-files-over-multiple-tarballs",level:3},{value:"Workflow",id:"workflow-1",level:4},{value:"Drawback",id:"drawback",level:4},{value:"Approach 1.1",id:"approach-11",level:3},{value:"Workflow",id:"workflow-2",level:4},{value:"Drawback",id:"drawback-1",level:4},{value:"Approach 2 (Single tarball creation under lock)",id:"approach-2-single-tarball-creation-under-lock",level:3},{value:"Drawback",id:"drawback-2",level:4},{value:"Approach 3 (Creating a checkpoint of the snapshot RocksDB under lock)",id:"approach-3-creating-a-checkpoint-of-the-snapshot-rocksdb-under-lock",level:3},{value:"Drawback",id:"drawback-3",level:4},{value:"Recommendations",id:"recommendations",level:2}];function c(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"om-bootstrapping-with-snapshots",children:"OM Bootstrapping with Snapshots"})}),"\n",(0,s.jsx)(t.h2,{id:"problem-statement",children:"Problem Statement"}),"\n",(0,s.jsx)(t.p,{children:"The current bootstrapping mechanism for OM has inconsistencies when dealing with Snapshotted OM RocksDBs. Bootstrapping\noccurs without locking mechanisms, and active transactions may still modify snapshots RocksDB during the process.\nThis can lead to a corrupted RocksDB instance on the follower OM post-bootstrapping. To resolve this, the bootstrapping\nprocess must operate on a consistent system state."}),"\n",(0,s.jsxs)(t.p,{children:["Jira Ticket: ",(0,s.jsx)(t.a,{href:"https://issues.apache.org/jira/browse/HDDS-12090",children:"HDDS-12090"})]}),"\n",(0,s.jsx)(t.h2,{id:"background-on-snapshots",children:"Background on Snapshots"}),"\n",(0,s.jsx)(t.h3,{id:"snapshot-operations",children:"Snapshot Operations"}),"\n",(0,s.jsx)(t.p,{children:"When a snapshot is taken on an Ozone bucket, the following steps occur:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["A RocksDB checkpoint of the active ",(0,s.jsx)(t.code,{children:"om.db"})," is created."]}),"\n",(0,s.jsxs)(t.li,{children:["Deleted entries are removed from the ",(0,s.jsx)(t.code,{children:"deletedKeyTable"})," and ",(0,s.jsx)(t.code,{children:"deletedDirTable"})," in the Active Object Store (AOS) RocksDB.\nThis is to just prevent the blocks from getting purged without checking for the key's presence in the correct snapshot in the snapshot chain."]}),"\n",(0,s.jsxs)(t.li,{children:["A new entry is added to the ",(0,s.jsx)(t.code,{children:"snapshotInfoTable"})," in the AOS RocksDB."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"current-bootstrap-model",children:"Current Bootstrap Model"}),"\n",(0,s.jsx)(t.p,{children:"The current model involves the follower OM initiating an HTTP request to the leader OM, which provides a consistent view of its state.\nBefore bucket snapshots were introduced, this process relied solely on an AOS RocksDB checkpoint. However, with snapshots, multiple RocksDB\ninstances (AOS RocksDB + snapshot RocksDBs) must be handled, complicating the process."}),"\n",(0,s.jsx)(t.h4,{id:"workflow",children:"Workflow"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Follower Initiation:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Sends an exclude list of files already copied in previous batches."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Leader Actions:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Creates an AOS RocksDB checkpoint."}),"\n",(0,s.jsxs)(t.li,{children:["Performs a directory walk through:","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"AOS RocksDB checkpoint directory."}),"\n",(0,s.jsx)(t.li,{children:"Snapshot RocksDB directories."}),"\n",(0,s.jsx)(t.li,{children:"Backup SST file directory (compaction backup directory)."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:"Identifies unique files to be copied in the next batch."}),"\n",(0,s.jsx)(t.li,{children:"Transfers files in batches, recreating hardlinks on the follower side as needed."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"issues-with-the-current-model",children:"Issues with the Current Model"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Active transactions during bootstrapping may modify snapshot RocksDBs, leading to inconsistencies."}),"\n",(0,s.jsx)(t.li,{children:"Partial data copies can occur when double-buffer flushes or other snapshot-related operations are in progress."}),"\n",(0,s.jsx)(t.li,{children:"Large snapshot data sizes (often in GBs) require multi-batch transfers, increasing the risk of data corruption."}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"proposed-fixes",children:"Proposed Fixes"}),"\n",(0,s.jsx)(t.h3,{id:"locking-the-snapshot-cache",children:"Locking the Snapshot Cache"}),"\n",(0,s.jsx)(t.p,{children:"Snapshot Cache is the class which is responsible for maintaining all RocksDB handles corresponding to a snapshot.\nThe RocksDB handles are closed by the snapshot cache from time to time if there are no references of the\nRocksDB being used by any of the threads in the system. Hence any operation on a snapshot would go through the snapshot\ncache increasing the reference count of that snapshot. Implementing a lock for this snapshot cache would prevent any newer\nthreads from requesting a snapshot RocksDB handle from the snapshot cache. Thus any operation under this lock will have a\nconsistent view of the entire snapshot. The only downside to this is that it would block the double buffer thread,\nhence any operation performed under this thread has to be lightweight so that it doesn't end up running for a long\nperiod of time. (P.S. With Sumit's implementation of optimized Gatekeeping model and getting rid of double buffer from\nOM would result in only blocking the snapshot operations which should be fine since these operations are only fired by background threads.)"}),"\n",(0,s.jsx)(t.p,{children:"With the above implementation of a lock there is a way to get a consistent snapshot of the entire OM. Now lets dive into various approaches to overall bootstrap flow."}),"\n",(0,s.jsx)(t.h3,{id:"approach-1-batching-files-over-multiple-tarballs",children:"Approach 1 (Batching files over multiple tarballs)"}),"\n",(0,s.jsx)(t.p,{children:"This approach builds the current model by introducing size thresholds to manage locks and data transfers more efficiently."}),"\n",(0,s.jsx)(t.h4,{id:"workflow-1",children:"Workflow"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Follower Initiation:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Sends an exclude list of previously copied files (identified by ",(0,s.jsx)(t.code,{children:"inodeId"}),")."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Leader Directory Walk:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Walks through AOS RocksDB, snapshot RocksDBs, and backup SST directories to identify files to transfer."}),"\n",(0,s.jsx)(t.li,{children:"Compares against the exclude list to avoid duplicate transfers."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["If the total size of files to be copied is more than ",(0,s.jsx)(t.code,{children:"ozone.om.ratis.snapshot.lock.max.total.size.threshold"})," then the\nfiles would be directly sent over the stream as a tarball where the name of the files is the inodeId of the file."]}),"\n",(0,s.jsxs)(t.li,{children:["If the total size of files to be copied is less than equal to ",(0,s.jsx)(t.code,{children:"ozone.om.ratis.snapshot.lock.max.total.size.threshold"}),"\nthen the snapshot cache lock is taken after waiting for the snapshot cache to completely get empty (No snapshot RocksDB should be open). Under the lock following operations would be performed:","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Take the AOS RocksDB checkpoint."}),"\n",(0,s.jsx)(t.li,{children:"A complete directory walk is done on AOS checkpoint RocksDB directory + all the snapshot RocksDB directories + backup sst\nfile directory (compaction log directory) to figure out all the files to be copied and any file already present in the exclude list would be excluded."}),"\n",(0,s.jsx)(t.li,{children:"These files are added to the tarball where again the name of the file would be the inodeId of the file."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:"As the files are being iterated the path of each file and their corresponding inodeIds would be tracked. When it is the\nlast batch this map would also be written as a text file in the final tarball to recreate all the hardlinks on the follower node."}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"drawback",children:"Drawback"}),"\n",(0,s.jsx)(t.p,{children:"The only drawback with this approach is that we might end up sending more data over the network because some sst files sent\nover the network could have been replaced because of compaction running concurrently on the active object store. But at the\nsame time since the entire bootstrap operation is supposed to finish in the order of a few minutes, the amount of extra data\nwould be really minimal assuming we could utmost write 30 MBs of data assuming there are 30000 keys written in 2 mins each\nkey would be around 1 KB."}),"\n",(0,s.jsx)(t.h3,{id:"approach-11",children:"Approach 1.1"}),"\n",(0,s.jsx)(t.p,{children:"This approach builds on the approach1 where along with introducing size thresholds under locks manage locks, we only rely\non the number of files changed under the snapshot directory as the threshold."}),"\n",(0,s.jsx)(t.h4,{id:"workflow-2",children:"Workflow"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Follower Initiation:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["Sends an exclude list of previously copied files (identified by ",(0,s.jsx)(t.code,{children:"inodeId"}),")."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Leader Directory Walk:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Walks through AOS RocksDB, snapshot RocksDBs, and backup SST directories to identify files to transfer."}),"\n",(0,s.jsx)(t.li,{children:"Compares against the exclude list to avoid duplicate transfers."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["If either the total size to be copied or the total number of files under the snapshot RocksDB directory to be copied is\nmore than ",(0,s.jsx)(t.code,{children:"ozone.om.ratis.snapshot.max.total.sst.size"})," respectively then the files would be directly sent over the stream as\na tarball where the name of the files is the inodeId of the file."]}),"\n",(0,s.jsxs)(t.li,{children:["If the total file size to be copied under the snapshot RocksDB directory is less than or equal to ",(0,s.jsx)(t.code,{children:"ozone.om.ratis.snapshot.max.total.sst.size"}),"\nthen the snapshot cache lock is taken after waiting for the snapshot cache to completely get empty (No snapshot RocksDB should be open).\nUnder the lock following operations would be performed:","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Take the AOS RocksDB checkpoint."}),"\n",(0,s.jsx)(t.li,{children:"A complete directory walk is done on all the snapshot RocksDB directories to figure out all the files to be copied and any file already present in the exclude list would be excluded."}),"\n",(0,s.jsx)(t.li,{children:"Hard links of these files are added to tmp directory on the leader."}),"\n",(0,s.jsx)(t.li,{children:"Exit lock"}),"\n",(0,s.jsx)(t.li,{children:"After the lock all files under the tmp directory, AOS RocksDB checkpoint directory and compaction backup directory have to be written to the tarball. As the files are being iterated the path of each file and their corresponding inodeIds would be tracked. Since this is the last batch this map would also be written as a text file in the final tarball to recreate all the hardlinks on the follower node."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"drawback-1",children:"Drawback"}),"\n",(0,s.jsx)(t.p,{children:"The drawbacks for this approach are the same as approach 1, but here we are optimizing on the amount of time lock is held\nby performing very lightweight operations under the lock. So this is a more optimal approach since it minimises the lock wait time on other threads."}),"\n",(0,s.jsx)(t.h3,{id:"approach-2-single-tarball-creation-under-lock",children:"Approach 2 (Single tarball creation under lock)"}),"\n",(0,s.jsx)(t.p,{children:"The approach 2 here proposes to create a single tarball file on the disk and stream the chunks of the tarball over multiple\nhttp batch request from the follower."}),"\n",(0,s.jsx)(t.p,{children:"Following is the flow for creating the tarball:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["Snapshot cache lock is taken after waiting for the snapshot cache to become completely empty (No snapshot RocksDB should be open). Under the lock following operations would be performed:","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Take the AOS RocksDB checkpoint."}),"\n",(0,s.jsx)(t.li,{children:"A complete directory walk is done on AOS RocksDB directory + all the snapshot RocksDB directories + backup sst file\ndirectory (compaction log directory) to figure out all the files to be copied to create a single tarball."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:"This tarball should be streamed batch by batch to the follower in a paginated fashion."}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"drawback-2",children:"Drawback"}),"\n",(0,s.jsx)(t.p,{children:"The drawback with this approach is that the double buffer would be blocked for a really long time if there is a lot of data to be tarballed. If the total snapshot size of the OM dbs put together is 1 TB, considering the tarball writes go to an NVMe and considering the write throughput for an NVMe drive is around 5 GB/sec then the tarball write might take a total of 1024/5 secs = 3 mins. Blocking the double buffer thread for 3 mins seems to be a bad idea, but at the same time this would only happen if there is snapshot operation in flight or in the double buffer queue already."}),"\n",(0,s.jsx)(t.h3,{id:"approach-3-creating-a-checkpoint-of-the-snapshot-rocksdb-under-lock",children:"Approach 3 (Creating a checkpoint of the snapshot RocksDB under lock)"}),"\n",(0,s.jsx)(t.p,{children:"The approach 3 here proposes to create a RocksDB checkpoint for each and every snapshot RocksDB in the system along with\nthe AOS RocksDB under the snapshot cache lock. Outside of the lock we could either create a single tarball file as done\nin approach 2 or stream the files in batches as multiple tarball file similar to approach 1 to the follower."}),"\n",(0,s.jsx)(t.p,{children:"Following is the flow for creating the tarball:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["Snapshot cache lock is taken after waiting for the snapshot cache to become completely empty (No snapshot RocksDB should be open).\nUnder the lock following operations would be performed:","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Take the AOS RocksDB checkpoint."}),"\n",(0,s.jsx)(t.li,{children:"Take RocksDB checkpoint of each and every snapshot in the system by iterating through the snapshotInfo table of AOS checkpoint RocksDB."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.li,{children:"Now the files in the checkpoint directories have to be streamed to the follower as done in either approach 1 or approach 2."}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"drawback-3",children:"Drawback"}),"\n",(0,s.jsx)(t.p,{children:"The drawback with this approach is that this would double the number of hardlinks in the file system which could have potential\nimpact on performance during bootstrap, considering the case in systems where the total number of files and hardlinks in the system\norder up to 5 million files."}),"\n",(0,s.jsx)(t.h2,{id:"recommendations",children:"Recommendations"}),"\n",(0,s.jsx)(t.p,{children:"Approach 1 is the most optimized solution as it balances the amount of time under the lock by minimising the amount of IO\nops inside the lock by introducing another threshold config to track this. Moreover, taking this approach will also need the\nmost minimal amount of code change as it doesn't differ from the current approach by much. While approach 2 might look simpler\nbut this would imply revamping the entire bootstrap logic currently in place and moreover this approach might increase the\ntotal amount of time inside the lock which would imply blocking the double buffer thread of extended amounts of time if it\ncomes to this situation, which approach 1 tries to avoid."}),"\n",(0,s.jsxs)(t.p,{children:["Final approach implemented is the ",(0,s.jsx)(t.strong,{children:"Approach 1.1"}),"."]})]})}function d(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);